{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook 1: Data Collection\n",
        "NYC Event Recommender - TimeOut NYC Scraper\n",
        "\n",
        "This notebook collects event data from TimeOut NYC including:\n",
        "- Basic event info (title, url, short description)\n",
        "- Full event descriptions\n",
        "- Pricing information (is_free)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "print(\"‚úÖ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data directories created\n"
          ]
        }
      ],
      "source": [
        "# Create data directories\n",
        "Path('../data/raw').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/test_datasets').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Data directories created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Fetch HTML Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fetching: https://www.timeout.com/newyork/things-to-do/things-to-do-in-nyc-this-weekend\n",
            "‚úÖ Saved HTML to ../data/raw/timeout_page_20251107.html\n",
            "üìä HTML size: 761,441 characters\n"
          ]
        }
      ],
      "source": [
        "# TimeOut NYC Things To Do page\n",
        "URL = 'https://www.timeout.com/newyork/things-to-do/things-to-do-in-nyc-this-weekend'\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "}\n",
        "\n",
        "print(f\"üîÑ Fetching: {URL}\")\n",
        "response = requests.get(URL, headers=HEADERS)\n",
        "response.raise_for_status()\n",
        "html_content = response.text\n",
        "\n",
        "# Save raw HTML\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "html_file = f'../data/raw/timeout_page_{today}.html'\n",
        "with open(html_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"‚úÖ Saved HTML to {html_file}\")\n",
        "print(f\"üìä HTML size: {len(html_content):,} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Parse Events from Listing Page\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 92 event cards\n",
            "\n",
            "‚úÖ Successfully parsed 90 events\n"
          ]
        }
      ],
      "source": [
        "def parse_events(html_content):\n",
        "    \"\"\"Parse event data from HTML - TimeOut NYC specific\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    events = []\n",
        "    \n",
        "    # TimeOut NYC uses <article class=\"tile\"> for events\n",
        "    event_cards = soup.find_all('article', class_=re.compile(r'tile|article', re.I))\n",
        "    \n",
        "    print(f\"Found {len(event_cards)} event cards\")\n",
        "    \n",
        "    # Skip the first card (it's a header)\n",
        "    for idx, card in enumerate(event_cards[1:], start=1):\n",
        "        try:\n",
        "            # Extract title\n",
        "            title_elem = card.find('h3') or card.find(['h2', 'h4'])\n",
        "            title = title_elem.get_text(strip=True) if title_elem else None\n",
        "            \n",
        "            if not title or len(title) < 3:\n",
        "                continue\n",
        "            \n",
        "            # Remove leading numbers from title\n",
        "            title = re.sub(r'^\\d+\\.\\s*', '', title).strip()\n",
        "            \n",
        "            # Extract URL\n",
        "            link_elem = card.find('a', href=True)\n",
        "            url = link_elem['href'] if link_elem else \"\"\n",
        "            if url and not url.startswith('http'):\n",
        "                url = f\"https://www.timeout.com{url}\"\n",
        "            \n",
        "            # Extract short description\n",
        "            desc_paragraphs = card.find_all('p')\n",
        "            if desc_paragraphs:\n",
        "                description = ' '.join([p.get_text(strip=True) for p in desc_paragraphs if p.get_text(strip=True)])\n",
        "            else:\n",
        "                description = title\n",
        "            \n",
        "            if not description or len(description) < 10:\n",
        "                description = title\n",
        "            \n",
        "            # Add event to list\n",
        "            events.append({\n",
        "                'event_id': f'evt_{len(events)+1:03d}',\n",
        "                'title': title,\n",
        "                'description': description,\n",
        "                'url': url,\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing event {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return events\n",
        "\n",
        "# Parse events\n",
        "events = parse_events(html_content)\n",
        "print(f\"\\n‚úÖ Successfully parsed {len(events)} events\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created DataFrame with 90 events\n",
            "\n",
            "Columns: ['event_id', 'title', 'description', 'url']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evt_001</td>\n",
              "      <td>The NY Comedy Festival</td>\n",
              "      <td>The New York Comedy Festival is where the best...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/the-ny-co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evt_002</td>\n",
              "      <td>The Other Art Fair Brooklyn</td>\n",
              "      <td>Connect with artists in-person and explore¬†hun...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evt_003</td>\n",
              "      <td>Canstruction</td>\n",
              "      <td>This annual cans-for-a-cause competitionpitsar...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>evt_004</td>\n",
              "      <td>Queer History Walking Tour</td>\n",
              "      <td>This fall, explore¬†the long and rich history o...</td>\n",
              "      <td>https://www.timeout.com/newyork/lgbtq/queer-hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>evt_005</td>\n",
              "      <td>Cheese Week</td>\n",
              "      <td>New Yorkers, prepare to get a littleextra chee...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/dairy-lov...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  event_id                        title  \\\n",
              "0  evt_001       The NY Comedy Festival   \n",
              "1  evt_002  The Other Art Fair Brooklyn   \n",
              "2  evt_003                 Canstruction   \n",
              "3  evt_004   Queer History Walking Tour   \n",
              "4  evt_005                  Cheese Week   \n",
              "\n",
              "                                         description  \\\n",
              "0  The New York Comedy Festival is where the best...   \n",
              "1  Connect with artists in-person and explore¬†hun...   \n",
              "2  This annual cans-for-a-cause competitionpitsar...   \n",
              "3  This fall, explore¬†the long and rich history o...   \n",
              "4  New Yorkers, prepare to get a littleextra chee...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.timeout.com/newyork/news/the-ny-co...  \n",
              "1  https://www.timeout.com/newyork/things-to-do/t...  \n",
              "2  https://www.timeout.com/newyork/things-to-do/c...  \n",
              "3  https://www.timeout.com/newyork/lgbtq/queer-hi...  \n",
              "4  https://www.timeout.com/newyork/news/dairy-lov...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(events)\n",
        "print(f\"‚úÖ Created DataFrame with {len(df)} events\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Download Individual Event Pages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Downloading 90 event pages in parallel...\n",
            "üìÅ Saving to: ../data/raw/event_pages_20251107\n",
            "\n",
            "  ‚úÖ Downloaded 10/90 pages...\n",
            "  ‚úÖ Downloaded 20/90 pages...\n",
            "  ‚úÖ Downloaded 30/90 pages...\n",
            "  ‚úÖ Downloaded 40/90 pages...\n",
            "  ‚úÖ Downloaded 50/90 pages...\n",
            "  ‚úÖ Downloaded 60/90 pages...\n",
            "  ‚úÖ Downloaded 70/90 pages...\n",
            "  ‚úÖ Downloaded 80/90 pages...\n",
            "  ‚úÖ Downloaded 90/90 pages...\n",
            "\n",
            "‚úÖ Successfully downloaded 90/90 event pages\n",
            "üìä Success rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "def download_event_page(event_id, url, headers, output_dir):\n",
        "    \"\"\"Download a single event page and save it to disk\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Create safe filename from URL\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', url.split('/')[-1])[:50]\n",
        "        filename = f\"{event_id}_{safe_title}.html\"\n",
        "        filepath = output_dir / filename\n",
        "        \n",
        "        # Save HTML\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)\n",
        "        \n",
        "        return event_id, str(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Error downloading {url}: {e}\")\n",
        "        return event_id, None\n",
        "\n",
        "# Create directory for event HTML files\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "html_dir = Path(f'../data/raw/event_pages_{today}')\n",
        "html_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üîÑ Downloading {len(df)} event pages in parallel...\")\n",
        "print(f\"üìÅ Saving to: {html_dir}\\n\")\n",
        "\n",
        "# Download all pages in parallel\n",
        "downloaded_files = {}\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {\n",
        "        executor.submit(download_event_page, row['event_id'], row['url'], HEADERS, html_dir): row['event_id']\n",
        "        for _, row in df.iterrows()\n",
        "    }\n",
        "    \n",
        "    completed = 0\n",
        "    for future in as_completed(futures):\n",
        "        completed += 1\n",
        "        event_id, filepath = future.result()\n",
        "        if filepath:\n",
        "            downloaded_files[event_id] = filepath\n",
        "        \n",
        "        if completed % 10 == 0 or completed == len(df):\n",
        "            print(f\"  ‚úÖ Downloaded {completed}/{len(df)} pages...\")\n",
        "\n",
        "# Add filepath column to DataFrame\n",
        "df['html_filepath'] = df['event_id'].map(downloaded_files)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully downloaded {len(downloaded_files)}/{len(df)} event pages\")\n",
        "print(f\"üìä Success rate: {len(downloaded_files)/len(df)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Extract Long Descriptions and Pricing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Extracting long descriptions and pricing info...\n",
            "\n",
            "‚úÖ Extracted descriptions: 90/90\n",
            "‚úÖ Detected pricing: 67/90\n",
            "\n",
            "üí∞ Pricing: 23 free, 44 paid, 23 unknown\n",
            "üìä Avg length: 2300 chars\n",
            "\n",
            "================================================================================\n",
            "SAMPLE: The NY Comedy Festival\n",
            "Is Free: None\n",
            "Description: The New York Comedy Festival(NYCF), the country‚Äôs largest and longest-running annual comedy festival, will return for its 21st edition this November, with over 200 comedians across 100 shows at iconic NYC venues likeCarnegie Hall,Madison Square Garden, theBeacon TheatreandTown HallfromFriday, Novemb...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def extract_event_details_from_file(html_filepath):\n",
        "    \"\"\"Extract long_description and is_free from event HTML\"\"\"\n",
        "    if not html_filepath or not Path(html_filepath).exists():\n",
        "        return \"\", True  # Assume free if file doesn't exist\n",
        "    \n",
        "    try:\n",
        "        with open(html_filepath, 'r', encoding='utf-8') as f:\n",
        "            soup = BeautifulSoup(f.read(), 'lxml')\n",
        "        \n",
        "        # Check price section first\n",
        "        is_free = None\n",
        "        price_section = soup.find('div', attrs={'data-section': 'price'})\n",
        "        if price_section:\n",
        "            price_text = price_section.get_text(strip=True).lower()\n",
        "            if 'free' in price_text:\n",
        "                is_free = True\n",
        "            elif re.search(r'\\$\\d+', price_text):\n",
        "                is_free = False\n",
        "        \n",
        "        # Clean up soup\n",
        "        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript']):\n",
        "            tag.decompose()\n",
        "        \n",
        "        # Extract long description\n",
        "        content_div = soup.find('div', id='content') or soup.find('div', class_=re.compile(r'contentAnnotation', re.I))\n",
        "        long_desc = \"\"\n",
        "        \n",
        "        if content_div:\n",
        "            paragraphs = content_div.find_all('p')\n",
        "            valid_paragraphs = []\n",
        "            \n",
        "            for p in paragraphs:\n",
        "                text = p.get_text(strip=True)\n",
        "                if len(text) < 20 or text.startswith('RECOMMENDED:') or 'View this post on Instagram' in text:\n",
        "                    continue\n",
        "                \n",
        "                # Fallback price detection if not found in price section\n",
        "                if is_free is None:\n",
        "                    text_lower = text.lower()\n",
        "                    if 'free' in text_lower or 'no admission' in text_lower:\n",
        "                        is_free = True\n",
        "                    elif '$' in text and re.search(r'\\$\\d+', text):\n",
        "                        is_free = False\n",
        "                \n",
        "                valid_paragraphs.append(text)\n",
        "            \n",
        "            long_desc = re.sub(r'\\s+', ' ', ' '.join(valid_paragraphs)).strip()[:5000]\n",
        "        \n",
        "        # If no price info found anywhere, assume it's free\n",
        "        if is_free is None:\n",
        "            is_free = True\n",
        "        \n",
        "        return long_desc, is_free\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", True  # Assume free on error\n",
        "\n",
        "\n",
        "# Extract details from all downloaded pages\n",
        "print(\"üîÑ Extracting long descriptions and pricing info...\\n\")\n",
        "\n",
        "results = df['html_filepath'].apply(lambda path: extract_event_details_from_file(path) if path else (\"\", None))\n",
        "df['long_description'] = results.apply(lambda x: x[0])\n",
        "df['is_free'] = results.apply(lambda x: x[1])\n",
        "\n",
        "# Statistics\n",
        "successful = (df['long_description'] != '').sum()\n",
        "has_pricing = df['is_free'].notna().sum()\n",
        "free = (df['is_free'] == True).sum()\n",
        "paid = (df['is_free'] == False).sum()\n",
        "\n",
        "print(f\"‚úÖ Extracted descriptions: {successful}/{len(df)}\")\n",
        "print(f\"‚úÖ Detected pricing: {has_pricing}/{len(df)}\")\n",
        "print(f\"\\nüí∞ Pricing: {free} free, {paid} paid, {len(df) - has_pricing} unknown\")\n",
        "print(f\"üìä Avg length: {df[df['long_description'] != '']['long_description'].str.len().mean():.0f} chars\")\n",
        "\n",
        "# Sample\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"SAMPLE: {df.iloc[0]['title']}\")\n",
        "print(f\"Is Free: {df.iloc[0]['is_free']}\")\n",
        "print(f\"Description: {df.iloc[0]['long_description'][:300]}...\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Save Final Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved 90 events to ../data/raw/timeout_events_20251107.csv\n",
            "\n",
            "üìã Final columns: ['event_id', 'title', 'description', 'url', 'long_description', 'is_free']\n",
            "\n",
            "üéØ Dataset ready for processing in Notebook 2!\n"
          ]
        }
      ],
      "source": [
        "# Drop html_filepath column (not needed for later use)\n",
        "df_final = df.drop(columns=['html_filepath'])\n",
        "\n",
        "# Save to CSV\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "output_file = f'../data/raw/timeout_events_{today}.csv'\n",
        "df_final.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(df_final)} events to {output_file}\")\n",
        "print(f\"\\nüìã Final columns: {list(df_final.columns)}\")\n",
        "print(f\"\\nüéØ Dataset ready for processing in Notebook 2!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Data Preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset Shape: (90, 6)\n",
            "\n",
            "First 3 events:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "      <th>long_description</th>\n",
              "      <th>is_free</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evt_001</td>\n",
              "      <td>The NY Comedy Festival</td>\n",
              "      <td>The New York Comedy Festival is where the best...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/the-ny-co...</td>\n",
              "      <td>The New York Comedy Festival(NYCF), the countr...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evt_002</td>\n",
              "      <td>The Other Art Fair Brooklyn</td>\n",
              "      <td>Connect with artists in-person and explore¬†hun...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/t...</td>\n",
              "      <td>Connect with artists in-person and explore hun...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evt_003</td>\n",
              "      <td>Canstruction</td>\n",
              "      <td>This annual cans-for-a-cause competitionpitsar...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/c...</td>\n",
              "      <td>This annual cans-for-a-cause competitionpitsar...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  event_id                        title  \\\n",
              "0  evt_001       The NY Comedy Festival   \n",
              "1  evt_002  The Other Art Fair Brooklyn   \n",
              "2  evt_003                 Canstruction   \n",
              "\n",
              "                                         description  \\\n",
              "0  The New York Comedy Festival is where the best...   \n",
              "1  Connect with artists in-person and explore¬†hun...   \n",
              "2  This annual cans-for-a-cause competitionpitsar...   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://www.timeout.com/newyork/news/the-ny-co...   \n",
              "1  https://www.timeout.com/newyork/things-to-do/t...   \n",
              "2  https://www.timeout.com/newyork/things-to-do/c...   \n",
              "\n",
              "                                    long_description is_free  \n",
              "0  The New York Comedy Festival(NYCF), the countr...    None  \n",
              "1  Connect with artists in-person and explore hun...   False  \n",
              "2  This annual cans-for-a-cause competitionpitsar...    True  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display final dataset\n",
        "print(f\"üìä Dataset Shape: {df_final.shape}\")\n",
        "print(f\"\\nFirst 3 events:\")\n",
        "df_final.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚úÖ‚úÖ‚úÖ Notebook 1 Complete!\n",
        "\n",
        "**What we collected:**\n",
        "- Event titles and URLs\n",
        "- Short descriptions (from listing page)\n",
        "- Long descriptions (from individual pages)\n",
        "- Pricing information (is_free)\n",
        "\n",
        "**Next step:** Open `02_data_processing_and_vectordb.ipynb` to process this data and create embeddings!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
