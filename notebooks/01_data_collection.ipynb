{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Data Collection\n",
        "\n",
        "**Objectives:**\n",
        "- Scrape TimeOut NYC's \"Things to Do This Weekend\" page\n",
        "- Parse event listings (title, description, date, category, price, location)\n",
        "- Save raw data to CSV\n",
        "\n",
        "**Target:** 80+ events from TimeOut NYC\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install beautifulsoup4 requests lxml pandas python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports successful!\n",
            "✅ Data directories created\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Create data directories if they don't exist\n",
        "Path('../data/raw').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/test_datasets').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"✅ Imports successful!\")\n",
        "print(f\"✅ Data directories created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Web Scraping\n",
        "\n",
        "✅✅✅ **Important:** We're setting up proper headers to avoid being blocked by the website.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch HTML Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully fetched page (Status: 200)\n",
            "✅ Content length: 843320 characters\n",
            "✅ HTML saved to: ../data/raw/timeout_page_20251019.html\n",
            "✅ You can now inspect it to see the structure!\n",
            "\n",
            "✅✅✅ Page fetched and saved! Ready to parse.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_URL = \"https://www.timeout.com/newyork/things-to-do/things-to-do-in-nyc-this-weekend\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\"\n",
        "}\n",
        "\n",
        "def fetch_page(url, headers):\n",
        "    \"\"\"Fetch HTML content from URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        print(f\"✅ Successfully fetched page (Status: {response.status_code})\")\n",
        "        print(f\"✅ Content length: {len(response.text)} characters\")\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Error fetching page: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fetch the page\n",
        "html_content = fetch_page(BASE_URL, HEADERS)\n",
        "\n",
        "if html_content:\n",
        "    # Save HTML to local file for inspection/debugging\n",
        "    today = datetime.now().strftime('%Y%m%d')\n",
        "    html_file = f'../data/raw/timeout_page_{today}.html'\n",
        "    \n",
        "    with open(html_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "    \n",
        "    print(f\"✅ HTML saved to: {html_file}\")\n",
        "    print(f\"✅ You can now inspect it to see the structure!\")\n",
        "    print(\"\\n✅✅✅ Page fetched and saved! Ready to parse.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Debug HTML Structure\n",
        "\n",
        "✅✅✅ **Let's inspect the HTML to find the right selectors:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 103 event cards\n",
            "\n",
            "✅ Successfully parsed 101 events\n"
          ]
        }
      ],
      "source": [
        "def parse_events(html_content):\n",
        "    \"\"\"Parse event data from HTML - TimeOut NYC specific\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    events = []\n",
        "    \n",
        "    # TimeOut NYC uses <article class=\"tile\"> for events\n",
        "    event_cards = soup.find_all('article', class_=re.compile(r'tile|article', re.I))\n",
        "    \n",
        "    print(f\"Found {len(event_cards)} event cards\")\n",
        "    \n",
        "    # Skip the first card (it's a header)\n",
        "    for idx, card in enumerate(event_cards[1:], start=1):\n",
        "        try:\n",
        "            # Extract title from <h3> inside <a>\n",
        "            title_elem = card.find('h3')\n",
        "            if not title_elem:\n",
        "                title_elem = card.find(['h2', 'h4'])\n",
        "            title = title_elem.get_text(strip=True) if title_elem else None\n",
        "            \n",
        "            if not title or len(title) < 3:\n",
        "                continue\n",
        "            \n",
        "            # Extract URL from <a> tag\n",
        "            link_elem = card.find('a', href=True)\n",
        "            url = link_elem['href'] if link_elem else \"\"\n",
        "            if url and not url.startswith('http'):\n",
        "                url = f\"https://www.timeout.com{url}\"\n",
        "            \n",
        "            # Extract description/summary (look for ALL p tags to get full description)\n",
        "            desc_paragraphs = card.find_all('p')\n",
        "            if desc_paragraphs:\n",
        "                # Combine all paragraph texts with space separator\n",
        "                description = ' '.join([p.get_text(strip=True) for p in desc_paragraphs if p.get_text(strip=True)])\n",
        "            else:\n",
        "                # Try finding divs with substantial text\n",
        "                content_div = card.find('div', class_=re.compile(r'content|description|summary', re.I))\n",
        "                if content_div:\n",
        "                    desc_paragraphs = content_div.find_all('p')\n",
        "                    description = ' '.join([p.get_text(strip=True) for p in desc_paragraphs if p.get_text(strip=True)])\n",
        "                else:\n",
        "                    description = title\n",
        "            \n",
        "            # Fallback to title if description is empty\n",
        "            if not description or len(description) < 10:\n",
        "                description = title\n",
        "            \n",
        "            # Extract category (often in data-layer or category tags)\n",
        "            category = \"General\"\n",
        "            # Look in data attributes\n",
        "            if link_elem and 'data-layer' in str(link_elem):\n",
        "                data_layer = str(link_elem.get('data-layer', ''))\n",
        "                if 'category' in data_layer.lower():\n",
        "                    # Extract category from data-layer JSON\n",
        "                    category_match = re.search(r'\"category\":\"([^\"]+)\"', data_layer)\n",
        "                    if category_match:\n",
        "                        category = category_match.group(1)\n",
        "            \n",
        "            \n",
        "            # Add event to list\n",
        "            events.append({\n",
        "                'event_id': f'evt_{len(events)+1:03d}',\n",
        "                'title': title,\n",
        "                'description': description,  # Limit description length\n",
        "                'url': url,\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing event {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return events\n",
        "\n",
        "# Parse events\n",
        "events = parse_events(html_content)\n",
        "print(f\"\\n✅ Successfully parsed {len(events)} events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Validation & Preview\n",
        "\n",
        "✅✅✅ **Let's check what we scraped:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total events scraped: 101\n",
            "\n",
            "Column names: ['event_id', 'title', 'description', 'url']\n",
            "\n",
            "Data shape: (101, 4)\n",
            "\n",
            "Missing values:\n",
            "event_id       0\n",
            "title          0\n",
            "description    0\n",
            "url            0\n",
            "dtype: int64\n",
            "\n",
            "================================================================================\n",
            "PREVIEW: First 5 Events\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evt_001</td>\n",
              "      <td>1.Open House New York</td>\n",
              "      <td>Admit it, are you a nosy New Yorker? Same here...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/open-hous...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evt_002</td>\n",
              "      <td>2.Tompkins Square Park Halloween Dog Parade</td>\n",
              "      <td>The Village Halloween Parade is fun and all, b...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evt_003</td>\n",
              "      <td>3.Gowanus Open Studios</td>\n",
              "      <td>Stroll through Gowanus to visit the art studio...</td>\n",
              "      <td>https://www.timeout.com/newyork/art/gowanus-op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>evt_004</td>\n",
              "      <td>4.\"Renoir’s Drawings\" at The Morgan</td>\n",
              "      <td>Renoir’s sketchbook is moving into the spotlig...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/renoirs-r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>evt_005</td>\n",
              "      <td>5.The Amazing Maize Maze</td>\n",
              "      <td>For the past 50 years, the Queens County Farm ...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/the-amazi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  event_id                                        title  \\\n",
              "0  evt_001                        1.Open House New York   \n",
              "1  evt_002  2.Tompkins Square Park Halloween Dog Parade   \n",
              "2  evt_003                       3.Gowanus Open Studios   \n",
              "3  evt_004          4.\"Renoir’s Drawings\" at The Morgan   \n",
              "4  evt_005                     5.The Amazing Maize Maze   \n",
              "\n",
              "                                         description  \\\n",
              "0  Admit it, are you a nosy New Yorker? Same here...   \n",
              "1  The Village Halloween Parade is fun and all, b...   \n",
              "2  Stroll through Gowanus to visit the art studio...   \n",
              "3  Renoir’s sketchbook is moving into the spotlig...   \n",
              "4  For the past 50 years, the Queens County Farm ...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.timeout.com/newyork/news/open-hous...  \n",
              "1  https://www.timeout.com/newyork/things-to-do/t...  \n",
              "2  https://www.timeout.com/newyork/art/gowanus-op...  \n",
              "3  https://www.timeout.com/newyork/news/renoirs-r...  \n",
              "4  https://www.timeout.com/newyork/news/the-amazi...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(events)\n",
        "\n",
        "# Display basic info\n",
        "print(f\"Total events scraped: {len(df)}\")\n",
        "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
        "print(f\"\\nData shape: {df.shape}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# Preview first 5 events\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREVIEW: First 5 Events\")\n",
        "print(\"=\"*80)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅✅✅ DATA QUALITY CHECKS:\n",
            "\n",
            "1. Events with valid titles: 101 / 101\n",
            "2. Events with descriptions: 101 / 101\n"
          ]
        }
      ],
      "source": [
        "# Check data quality\n",
        "print(\"\\n✅✅✅ DATA QUALITY CHECKS:\\n\")\n",
        "\n",
        "print(f\"1. Events with valid titles: {(df['title'] != 'No title').sum()} / {len(df)}\")\n",
        "print(f\"2. Events with descriptions: {(df['description'] != 'No description').sum()} / {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Raw Data to CSV\n",
        "\n",
        "✅✅✅ **Saving to:** `data/raw/timeout_events_YYYYMMDD.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data saved to: ../data/raw/timeout_events_20251019.csv\n",
            "✅ Total events saved: 101\n",
            "✅ File size: 83.56 KB\n",
            "\n",
            "✅ Verification: Successfully read back 101 events from CSV\n"
          ]
        }
      ],
      "source": [
        "# Generate filename with today's date\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "output_file = f'../data/raw/timeout_events_{today}.csv'\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"✅ Data saved to: {output_file}\")\n",
        "print(f\"✅ Total events saved: {len(df)}\")\n",
        "print(f\"✅ File size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
        "\n",
        "# Verify we can read it back\n",
        "verify_df = pd.read_csv(output_file)\n",
        "print(f\"\\n✅ Verification: Successfully read back {len(verify_df)} events from CSV\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "✅✅✅ **Notebook 1 Complete!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "NOTEBOOK 1 SUMMARY: DATA COLLECTION\n",
            "================================================================================\n",
            "\n",
            "✅✅✅ SUCCESSFULLY COMPLETED!\n",
            "\n",
            "📊 Events Collected: 101\n",
            "📁 Saved to: ../data/raw/timeout_events_20251019.csv\n",
            "🗂️  Columns: event_id, title, description, url\n",
            "\n",
            "📈 Summary Statistics:\n",
            "   - Events with descriptions: 101\n",
            "   - Events with URLs: 101\n",
            "\n",
            "✅ SUCCESS: Collected 101 events (target: 80+)\n",
            "\n",
            "📝 Next Step: Notebook 2 - Data Processing & Vector DB\n",
            "   - Extract baby_friendly metadata using LLM\n",
            "   - Generate embeddings with OpenAI\n",
            "   - Set up Qdrant vector database\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"NOTEBOOK 1 SUMMARY: DATA COLLECTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n✅✅✅ SUCCESSFULLY COMPLETED!\\n\")\n",
        "print(f\"📊 Events Collected: {len(df)}\")\n",
        "print(f\"📁 Saved to: {output_file}\")\n",
        "print(f\"🗂️  Columns: {', '.join(df.columns)}\")\n",
        "print(f\"\\n📈 Summary Statistics:\")\n",
        "print(f\"   - Events with descriptions: {(df['description'] != 'No description').sum()}\")\n",
        "print(f\"   - Events with URLs: {(df['url'] != '').sum()}\")\n",
        "\n",
        "if len(df) >= 80:\n",
        "    print(f\"\\n✅ SUCCESS: Collected {len(df)} events (target: 80+)\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  WARNING: Only collected {len(df)} events (target: 80+)\")\n",
        "    print(f\"   Consider scraping additional pages or sections\")\n",
        "\n",
        "print(f\"\\n📝 Next Step: Notebook 2 - Data Processing & Vector DB\")\n",
        "print(f\"   - Extract baby_friendly metadata using LLM\")\n",
        "print(f\"   - Generate embeddings with OpenAI\")\n",
        "print(f\"   - Set up Qdrant vector database\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ✅✅✅ Notebook 1 Complete!\n",
        "\n",
        "**What we accomplished:**\n",
        "1. ✅ Set up web scraping with proper headers\n",
        "2. ✅ Scraped TimeOut NYC event listings\n",
        "3. ✅ Parsed event data (title, description, date, category, price, location, url)\n",
        "4. ✅ Validated data quality\n",
        "5. ✅ Saved raw data to CSV: `data/raw/timeout_events_YYYYMMDD.csv`\n",
        "\n",
        "**CSV Structure:**\n",
        "- `event_id`: Unique identifier\n",
        "- `title`: Event name\n",
        "- `description`: Event summary\n",
        "- `date`: When it happens\n",
        "- `category`: Type (Arts, Food, Outdoor, etc.)\n",
        "- `price`: free, $, $$, $$$\n",
        "- `location`: Neighborhood/venue\n",
        "- `url`: Link to full event page\n",
        "- `scraped_at`: Timestamp\n",
        "\n",
        "**Next Steps:**\n",
        "- Move to **Notebook 2: Data Processing & Vector DB**\n",
        "- Extract `baby_friendly` metadata using GPT-4\n",
        "- Generate embeddings with OpenAI\n",
        "- Set up Qdrant vector database\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
