{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Data Collection\n",
        "\n",
        "**Objectives:**\n",
        "- Scrape TimeOut NYC's \"Things to Do This Weekend\" page\n",
        "- Parse event listings (title, description, date, category, price, location)\n",
        "- Save raw data to CSV\n",
        "\n",
        "**Target:** 80+ events from TimeOut NYC\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install beautifulsoup4 requests lxml pandas python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful!\n",
            "‚úÖ Data directories created\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Create data directories if they don't exist\n",
        "Path('../data/raw').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
        "Path('../data/test_datasets').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n",
        "print(f\"‚úÖ Data directories created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Web Scraping\n",
        "\n",
        "‚úÖ‚úÖ‚úÖ **Important:** We're setting up proper headers to avoid being blocked by the website.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch HTML Content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Event Pages\n",
        "\n",
        "Now we'll download the full HTML for each event page to extract detailed descriptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Downloading 90 event pages in parallel...\n",
            "üìÅ Saving to: ../data/raw/event_pages_20251106\n",
            "\n",
            "  ‚úÖ Downloaded 10/90 pages...\n",
            "  ‚úÖ Downloaded 20/90 pages...\n",
            "  ‚úÖ Downloaded 30/90 pages...\n",
            "  ‚úÖ Downloaded 40/90 pages...\n",
            "  ‚úÖ Downloaded 50/90 pages...\n",
            "  ‚úÖ Downloaded 60/90 pages...\n",
            "  ‚úÖ Downloaded 70/90 pages...\n",
            "  ‚úÖ Downloaded 80/90 pages...\n",
            "  ‚úÖ Downloaded 90/90 pages...\n",
            "\n",
            "‚úÖ Successfully downloaded 90/90 event pages\n",
            "üìä Success rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "def download_event_page(event_id, url, headers, output_dir):\n",
        "    \"\"\"Download a single event page and save it to disk\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Create safe filename from title\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', url.split('/')[-1])[:50]\n",
        "        filename = f\"{event_id}_{safe_title}.html\"\n",
        "        filepath = output_dir / filename\n",
        "        \n",
        "        # Save HTML\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)\n",
        "        \n",
        "        return event_id, str(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Error downloading {url}: {e}\")\n",
        "        return event_id, None\n",
        "\n",
        "# Create directory for event HTML files\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "html_dir = Path(f'../data/raw/event_pages_{today}')\n",
        "html_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üîÑ Downloading {len(df)} event pages in parallel...\")\n",
        "print(f\"üìÅ Saving to: {html_dir}\\n\")\n",
        "\n",
        "# Download all pages in parallel\n",
        "downloaded_files = {}\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # Submit all download tasks\n",
        "    futures = {\n",
        "        executor.submit(download_event_page, row['event_id'], row['url'], HEADERS, html_dir): row['event_id']\n",
        "        for _, row in df.iterrows()\n",
        "    }\n",
        "    \n",
        "    # Collect results\n",
        "    completed = 0\n",
        "    for future in as_completed(futures):\n",
        "        completed += 1\n",
        "        event_id, filepath = future.result()\n",
        "        if filepath:\n",
        "            downloaded_files[event_id] = filepath\n",
        "        \n",
        "        if completed % 10 == 0 or completed == len(df):\n",
        "            print(f\"  ‚úÖ Downloaded {completed}/{len(df)} pages...\")\n",
        "\n",
        "# Add filepath column to DataFrame\n",
        "df['html_filepath'] = df['event_id'].map(downloaded_files)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully downloaded {len(downloaded_files)}/{len(df)} event pages\")\n",
        "print(f\"üìä Success rate: {len(downloaded_files)/len(df)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Long Descriptions\n",
        "\n",
        "Extract the full description from each downloaded event page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Extracting long descriptions from downloaded pages...\n",
            "\n",
            "‚úÖ Extracted descriptions for 90/90 events\n",
            "üìä Average description length: 2281 characters\n",
            "\n",
            "================================================================================\n",
            "SAMPLE: First Event Long Description\n",
            "================================================================================\n",
            "The New York Comedy Festival(NYCF), the country‚Äôs largest and longest-running annual comedy festival, will return for its 21st edition this November, with over 200 comedians across 100 shows at iconic NYC venues likeCarnegie Hall,Madison Square Garden, theBeacon TheatreandTown HallfromFriday, Novemb...\n"
          ]
        }
      ],
      "source": [
        "def extract_long_description_from_file(html_filepath):\n",
        "    \"\"\"Extract long description from a downloaded HTML file\"\"\"\n",
        "    if not html_filepath or not Path(html_filepath).exists():\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        with open(html_filepath, 'r', encoding='utf-8') as f:\n",
        "            html_content = f.read()\n",
        "        \n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        \n",
        "        # Remove unwanted elements\n",
        "        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'noscript']):\n",
        "            element.decompose()\n",
        "        \n",
        "        # Find the content div - TimeOut uses id=\"content\" with contentAnnotation class\n",
        "        content_div = soup.find('div', id='content')\n",
        "        if not content_div:\n",
        "            # Fallback: look for div with contentAnnotation class\n",
        "            content_div = soup.find('div', class_=re.compile(r'contentAnnotation', re.I))\n",
        "        \n",
        "        if content_div:\n",
        "            # Extract all paragraph text\n",
        "            paragraphs = content_div.find_all('p')\n",
        "            long_desc_parts = []\n",
        "            \n",
        "            for p in paragraphs:\n",
        "                text = p.get_text(strip=True)\n",
        "                # Skip very short paragraphs\n",
        "                if len(text) < 20:\n",
        "                    continue\n",
        "                # Skip RECOMMENDED links\n",
        "                if text.startswith('RECOMMENDED:'):\n",
        "                    continue\n",
        "                # Skip social media links\n",
        "                if 'View this post on Instagram' in text or 'A post shared by' in text:\n",
        "                    continue\n",
        "                long_desc_parts.append(text)\n",
        "            \n",
        "            long_desc = ' '.join(long_desc_parts)\n",
        "            long_desc = re.sub(r'\\s+', ' ', long_desc).strip()\n",
        "            \n",
        "            if len(long_desc) > 100:\n",
        "                return long_desc[:5000]\n",
        "        \n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"üîÑ Extracting long descriptions from downloaded pages...\\n\")\n",
        "\n",
        "# Apply extraction function to all events\n",
        "df['long_description'] = df['html_filepath'].apply(\n",
        "    lambda filepath: extract_long_description_from_file(filepath) if filepath else \"\"\n",
        ")\n",
        "\n",
        "# Show statistics\n",
        "successful = (df['long_description'] != '').sum()\n",
        "avg_length = df[df['long_description'] != '']['long_description'].str.len().mean()\n",
        "\n",
        "print(f\"‚úÖ Extracted descriptions for {successful}/{len(df)} events\")\n",
        "print(f\"üìä Average description length: {avg_length:.0f} characters\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE: First Event Long Description\")\n",
        "print(\"=\"*80)\n",
        "first_desc = df.iloc[0]['long_description']\n",
        "print(f\"{first_desc[:300]}...\" if len(first_desc) > 300 else first_desc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully fetched page (Status: 200)\n",
            "‚úÖ Content length: 774788 characters\n",
            "‚úÖ HTML saved to: ../data/raw/timeout_page_20251106.html\n",
            "‚úÖ You can now inspect it to see the structure!\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ Page fetched and saved! Ready to parse.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "BASE_URL = \"https://www.timeout.com/newyork/things-to-do/things-to-do-in-nyc-this-weekend\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\"\n",
        "}\n",
        "\n",
        "def fetch_page(url, headers):\n",
        "    \"\"\"Fetch HTML content from URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        print(f\"‚úÖ Successfully fetched page (Status: {response.status_code})\")\n",
        "        print(f\"‚úÖ Content length: {len(response.text)} characters\")\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error fetching page: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fetch the page\n",
        "html_content = fetch_page(BASE_URL, HEADERS)\n",
        "\n",
        "if html_content:\n",
        "    # Save HTML to local file for inspection/debugging\n",
        "    today = datetime.now().strftime('%Y%m%d')\n",
        "    html_file = f'../data/raw/timeout_page_{today}.html'\n",
        "    \n",
        "    with open(html_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "    \n",
        "    print(f\"‚úÖ HTML saved to: {html_file}\")\n",
        "    print(f\"‚úÖ You can now inspect it to see the structure!\")\n",
        "    print(\"\\n‚úÖ‚úÖ‚úÖ Page fetched and saved! Ready to parse.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Debug HTML Structure\n",
        "\n",
        "‚úÖ‚úÖ‚úÖ **Let's inspect the HTML to find the right selectors:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 92 event cards\n",
            "\n",
            "‚úÖ Successfully parsed 90 events\n"
          ]
        }
      ],
      "source": [
        "def parse_events(html_content):\n",
        "    \"\"\"Parse event data from HTML - TimeOut NYC specific\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    events = []\n",
        "    \n",
        "    # TimeOut NYC uses <article class=\"tile\"> for events\n",
        "    event_cards = soup.find_all('article', class_=re.compile(r'tile|article', re.I))\n",
        "    \n",
        "    print(f\"Found {len(event_cards)} event cards\")\n",
        "    \n",
        "    # Skip the first card (it's a header)\n",
        "    for idx, card in enumerate(event_cards[1:], start=1):\n",
        "        try:\n",
        "            # Extract title from <h3> inside <a>\n",
        "            title_elem = card.find('h3')\n",
        "            if not title_elem:\n",
        "                title_elem = card.find(['h2', 'h4'])\n",
        "            title = title_elem.get_text(strip=True) if title_elem else None\n",
        "            \n",
        "            if not title or len(title) < 3:\n",
        "                continue\n",
        "            \n",
        "            # Extract URL from <a> tag\n",
        "            link_elem = card.find('a', href=True)\n",
        "            url = link_elem['href'] if link_elem else \"\"\n",
        "            if url and not url.startswith('http'):\n",
        "                url = f\"https://www.timeout.com{url}\"\n",
        "            \n",
        "            # Extract description/summary (look for ALL p tags to get full description)\n",
        "            desc_paragraphs = card.find_all('p')\n",
        "            if desc_paragraphs:\n",
        "                # Combine all paragraph texts with space separator\n",
        "                description = ' '.join([p.get_text(strip=True) for p in desc_paragraphs if p.get_text(strip=True)])\n",
        "            else:\n",
        "                # Try finding divs with substantial text\n",
        "                content_div = card.find('div', class_=re.compile(r'content|description|summary', re.I))\n",
        "                if content_div:\n",
        "                    desc_paragraphs = content_div.find_all('p')\n",
        "                    description = ' '.join([p.get_text(strip=True) for p in desc_paragraphs if p.get_text(strip=True)])\n",
        "                else:\n",
        "                    description = title\n",
        "            \n",
        "            # Fallback to title if description is empty\n",
        "            if not description or len(description) < 10:\n",
        "                description = title\n",
        "            \n",
        "            # Extract category (often in data-layer or category tags)\n",
        "            category = \"General\"\n",
        "            # Look in data attributes\n",
        "            if link_elem and 'data-layer' in str(link_elem):\n",
        "                data_layer = str(link_elem.get('data-layer', ''))\n",
        "                if 'category' in data_layer.lower():\n",
        "                    # Extract category from data-layer JSON\n",
        "                    category_match = re.search(r'\"category\":\"([^\"]+)\"', data_layer)\n",
        "                    if category_match:\n",
        "                        category = category_match.group(1)\n",
        "            \n",
        "            \n",
        "            # Add event to list\n",
        "            events.append({\n",
        "                'event_id': f'evt_{len(events)+1:03d}',\n",
        "                'title': title,\n",
        "                'description': description,  # Limit description length\n",
        "                'url': url,\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing event {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return events\n",
        "\n",
        "# Parse events\n",
        "events = parse_events(html_content)\n",
        "print(f\"\\n‚úÖ Successfully parsed {len(events)} events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Validation & Preview\n",
        "\n",
        "‚úÖ‚úÖ‚úÖ **Let's check what we scraped:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total events scraped: 90\n",
            "\n",
            "Column names: ['event_id', 'title', 'description', 'url']\n",
            "\n",
            "Data shape: (90, 4)\n",
            "\n",
            "Missing values:\n",
            "event_id       0\n",
            "title          0\n",
            "description    0\n",
            "url            0\n",
            "dtype: int64\n",
            "\n",
            "================================================================================\n",
            "PREVIEW: First 5 Events\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_id</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evt_001</td>\n",
              "      <td>1.The NY Comedy Festival</td>\n",
              "      <td>The New York Comedy Festival is where the best...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/the-ny-co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evt_002</td>\n",
              "      <td>2.The Other Art Fair Brooklyn</td>\n",
              "      <td>Connect with artists in-person and explore¬†hun...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evt_003</td>\n",
              "      <td>3.Canstruction</td>\n",
              "      <td>This annual cans-for-a-cause competitionpitsar...</td>\n",
              "      <td>https://www.timeout.com/newyork/things-to-do/c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>evt_004</td>\n",
              "      <td>4.Queer History Walking Tour</td>\n",
              "      <td>This fall, explore¬†the long and rich history o...</td>\n",
              "      <td>https://www.timeout.com/newyork/lgbtq/queer-hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>evt_005</td>\n",
              "      <td>5.Cheese Week</td>\n",
              "      <td>New Yorkers, prepare to get a littleextra chee...</td>\n",
              "      <td>https://www.timeout.com/newyork/news/dairy-lov...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  event_id                          title  \\\n",
              "0  evt_001       1.The NY Comedy Festival   \n",
              "1  evt_002  2.The Other Art Fair Brooklyn   \n",
              "2  evt_003                 3.Canstruction   \n",
              "3  evt_004   4.Queer History Walking Tour   \n",
              "4  evt_005                  5.Cheese Week   \n",
              "\n",
              "                                         description  \\\n",
              "0  The New York Comedy Festival is where the best...   \n",
              "1  Connect with artists in-person and explore¬†hun...   \n",
              "2  This annual cans-for-a-cause competitionpitsar...   \n",
              "3  This fall, explore¬†the long and rich history o...   \n",
              "4  New Yorkers, prepare to get a littleextra chee...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.timeout.com/newyork/news/the-ny-co...  \n",
              "1  https://www.timeout.com/newyork/things-to-do/t...  \n",
              "2  https://www.timeout.com/newyork/things-to-do/c...  \n",
              "3  https://www.timeout.com/newyork/lgbtq/queer-hi...  \n",
              "4  https://www.timeout.com/newyork/news/dairy-lov...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(events)\n",
        "\n",
        "# Display basic info\n",
        "print(f\"Total events scraped: {len(df)}\")\n",
        "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
        "print(f\"\\nData shape: {df.shape}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# Preview first 5 events\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREVIEW: First 5 Events\")\n",
        "print(\"=\"*80)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ‚úÖ‚úÖ DATA QUALITY CHECKS:\n",
            "\n",
            "1. Events with valid titles: 90 / 90\n",
            "2. Events with descriptions: 90 / 90\n"
          ]
        }
      ],
      "source": [
        "# Check data quality\n",
        "print(\"\\n‚úÖ‚úÖ‚úÖ DATA QUALITY CHECKS:\\n\")\n",
        "\n",
        "print(f\"1. Events with valid titles: {(df['title'] != 'No title').sum()} / {len(df)}\")\n",
        "print(f\"2. Events with descriptions: {(df['description'] != 'No description').sum()} / {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Raw Data to CSV\n",
        "\n",
        "‚úÖ‚úÖ‚úÖ **Saving to:** `data/raw/timeout_events_YYYYMMDD.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data saved to: ../data/raw/timeout_events_20251106.csv\n",
            "‚úÖ Total events saved: 90\n",
            "‚úÖ File size: 75.13 KB\n",
            "\n",
            "‚úÖ Verification: Successfully read back 90 events from CSV\n"
          ]
        }
      ],
      "source": [
        "# Generate filename with today's date\n",
        "today = datetime.now().strftime('%Y%m%d')\n",
        "output_file = f'../data/raw/timeout_events_{today}.csv'\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"‚úÖ Data saved to: {output_file}\")\n",
        "print(f\"‚úÖ Total events saved: {len(df)}\")\n",
        "print(f\"‚úÖ File size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
        "\n",
        "# Verify we can read it back\n",
        "verify_df = pd.read_csv(output_file)\n",
        "print(f\"\\n‚úÖ Verification: Successfully read back {len(verify_df)} events from CSV\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "‚úÖ‚úÖ‚úÖ **Notebook 1 Complete!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "NOTEBOOK 1 SUMMARY: DATA COLLECTION\n",
            "================================================================================\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ SUCCESSFULLY COMPLETED!\n",
            "\n",
            "üìä Events Collected: 90\n",
            "üìÅ Saved to: ../data/raw/timeout_events_20251106.csv\n",
            "üóÇÔ∏è  Columns: event_id, title, description, url\n",
            "\n",
            "üìà Summary Statistics:\n",
            "   - Events with descriptions: 90\n",
            "   - Events with URLs: 90\n",
            "\n",
            "‚úÖ SUCCESS: Collected 90 events (target: 80+)\n",
            "\n",
            "üìù Next Step: Notebook 2 - Data Processing & Vector DB\n",
            "   - Extract baby_friendly metadata using LLM\n",
            "   - Generate embeddings with OpenAI\n",
            "   - Set up Qdrant vector database\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"NOTEBOOK 1 SUMMARY: DATA COLLECTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n‚úÖ‚úÖ‚úÖ SUCCESSFULLY COMPLETED!\\n\")\n",
        "print(f\"üìä Events Collected: {len(df)}\")\n",
        "print(f\"üìÅ Saved to: {output_file}\")\n",
        "print(f\"üóÇÔ∏è  Columns: {', '.join(df.columns)}\")\n",
        "print(f\"\\nüìà Summary Statistics:\")\n",
        "print(f\"   - Events with descriptions: {(df['description'] != 'No description').sum()}\")\n",
        "print(f\"   - Events with URLs: {(df['url'] != '').sum()}\")\n",
        "\n",
        "if len(df) >= 80:\n",
        "    print(f\"\\n‚úÖ SUCCESS: Collected {len(df)} events (target: 80+)\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: Only collected {len(df)} events (target: 80+)\")\n",
        "    print(f\"   Consider scraping additional pages or sections\")\n",
        "\n",
        "print(f\"\\nüìù Next Step: Notebook 2 - Data Processing & Vector DB\")\n",
        "print(f\"   - Extract baby_friendly metadata using LLM\")\n",
        "print(f\"   - Generate embeddings with OpenAI\")\n",
        "print(f\"   - Set up Qdrant vector database\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ‚úÖ‚úÖ Notebook 1 Complete!\n",
        "\n",
        "**What we accomplished:**\n",
        "1. ‚úÖ Set up web scraping with proper headers\n",
        "2. ‚úÖ Scraped TimeOut NYC event listings\n",
        "3. ‚úÖ Parsed event data (title, description, date, category, price, location, url)\n",
        "4. ‚úÖ Validated data quality\n",
        "5. ‚úÖ Saved raw data to CSV: `data/raw/timeout_events_YYYYMMDD.csv`\n",
        "\n",
        "**CSV Structure:**\n",
        "- `event_id`: Unique identifier\n",
        "- `title`: Event name\n",
        "- `description`: Event summary\n",
        "- `date`: When it happens\n",
        "- `category`: Type (Arts, Food, Outdoor, etc.)\n",
        "- `price`: free, $, $$, $$$\n",
        "- `location`: Neighborhood/venue\n",
        "- `url`: Link to full event page\n",
        "- `scraped_at`: Timestamp\n",
        "\n",
        "**Next Steps:**\n",
        "- Move to **Notebook 2: Data Processing & Vector DB**\n",
        "- Extract `baby_friendly` metadata using GPT-4\n",
        "- Generate embeddings with OpenAI\n",
        "- Set up Qdrant vector database\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
